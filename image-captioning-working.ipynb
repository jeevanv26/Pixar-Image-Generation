{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8360485,"sourceType":"datasetVersion","datasetId":4968658},{"sourceId":75676,"sourceType":"datasetVersion","datasetId":42780}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\nfrom torch.utils.data import Dataset\nfrom torchtext.data import get_tokenizer\nimport requests\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport pickle\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_raw = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\ntokenizer       = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming image_processor, model, and tokenizer are already defined or imported elsewhere\n\ndef show_n_generate(source, greedy=True, model=model_raw):\n    # Check if the source is a URL or a local file path based on common URL patterns\n    if source.startswith(('http://', 'https://')):\n        # Load image from a URL\n        image = Image.open(requests.get(source, stream=True).raw)\n    else:\n        # Load image from a local file\n        image = Image.open(source)\n    \n    # Display the image\n    plt.imshow(image)\n    plt.show()\n\n    # Process image to get pixel values\n    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n    \n    # Text generation with the model\n    if greedy:\n        generated_ids = model.generate(pixel_values, max_new_tokens=30)\n    else:\n        generated_ids = model.generate(\n            pixel_values,\n            do_sample=True,\n            max_new_tokens=30,\n            top_k=5\n        )\n    \n    # Decode generated text\n    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    print(generated_text)\n\n# Example usage with a local file path\nlocal_path = \"/kaggle/input/natural-images/natural_images/dog/dog_0007.jpg\"\nshow_n_generate(local_path)\n","metadata":{},"execution_count":null,"outputs":[]}]}